{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vvtcn7AZZzPO"
      },
      "source": [
        "  # @staticmethod\n",
        "  # def gen_random(nstates=4, min_terminals=1, ntransitions=4):\n",
        "\n",
        "  #   if min_terminals > nstates:\n",
        "  #     raise Exception(\"nterminals > nstates\")\n",
        "\n",
        "  #   # Let's generate a bunch of states\n",
        "  #   states = [FSM() for _ in range(nstates)]\n",
        "  #   start = states[0]\n",
        "\n",
        "  #   # Add random transitions\n",
        "  #   prev_state = start\n",
        "  #   cnt_inputs = 0\n",
        "  #   for _ in range(ntransitions):\n",
        "  #     next_state = random.choice(states)\n",
        "  #     # Shall we create a new input or reuse an old one\n",
        "  #     if random.choice([True,False]):\n",
        "  #       val = cnt_inputs\n",
        "  #       cnt_inputs += 1\n",
        "  #     else:\n",
        "  #       val = random.randint(0,cnt_inputs+1)\n",
        "\n",
        "  #     prev_state.add_transition(val, [next_state])\n",
        "  #     prev_state = next_state\n",
        "\n",
        "  #   # Let's check how many terminal states we have already\n",
        "  #   count_terminals = 0\n",
        "  #   for state in states:\n",
        "  #     if state.is_final():\n",
        "  #       count_terminals += 1\n",
        "\n",
        "  #   # Let's add as many terminal states as needed to reach min_terminal\n",
        "  #   while count_terminals < min_terminals:\n",
        "  #     state = random.choice(states)\n",
        "  #     if not state.is_final():\n",
        "  #       state.set_final(True)\n",
        "  #       count_terminals += 1\n",
        "\n",
        "  #   # There is a risk we never terminate, let's prevent that\n",
        "  #   while len(list(start.bfs(max=1, max_depth=ntransitions))) == 0:\n",
        "  #     random.choice(states).set_final(True)\n",
        "\n",
        "  #   return start\n",
        "\n",
        "\n",
        "\n",
        "#def max_pad(exprs, val=0):\n",
        "#  exprs = list(exprs)\n",
        "#  max_len = len(max(exprs, key = lambda x: len(x)))\n",
        "#\n",
        "#  for expr in exprs:\n",
        "#    expr.extend([val]*(max_len-len(expr)))\n",
        "#  return exprs\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tzEZzc0vkgL3"
      },
      "source": [
        "import os\n",
        "import glob\n",
        "import numpy as np\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow import keras\n",
        "import tensorflow_probability as tfp\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import string\n",
        "import random\n",
        "from typing import Dict, List, Optional, Generator, Tuple\n",
        "from collections import deque\n",
        "import functools"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "997zz5Aa4gTJ"
      },
      "source": [
        "# Part 1 - Defining Languages\n",
        "\n",
        "Our goal is to escalete Chomsky hierarchy of language and see which model\n",
        "performs best (i.e. speed, precision, etc.) at learning which language.\n",
        "\n",
        "As a first step we create few Machine classes. Each Machine represent a class of languages and provide a `gen` and `parse` method to respectively generate expressions and parse to check their well-formedness.\n",
        "\n",
        "The FSM (Finite State Machine) class is used to describe regular languages and generate samples."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ig_zI15Nkv4-"
      },
      "source": [
        "class Machine:\n",
        "  def gen(self, max_samples: Optional[int] = None, max_length: Optional[int] = None) -> Generator[List[int], None, None]:\n",
        "    \"\"\"\n",
        "    Generates up to max_samples well formed expressions beloging to the underlying\n",
        "    language.\n",
        "\n",
        "    max_samples: max number of samples to generate. Note that this value is an\n",
        "      upper limit and there is no lower limit on the number of expressions\n",
        "      generated (i.e. the machine might generate from 0 to max_samples expressions)\n",
        "      If None generate as many expressions as possible (potentially infinite).\n",
        "    max_length: max length of the expressions to be enforced, if None generate\n",
        "      expressions of any length\n",
        "    \"\"\"\n",
        "    raise NotImplementedError(\"Method gen not implemented\")\n",
        "\n",
        "  def parse(self, expr: List[int], stop_symbol: Optional[int] = None) -> bool:\n",
        "    \"\"\"\n",
        "    Parse an expression and return True if it is well formed (i.e. it belongs to\n",
        "    the language encoded by this Machine and it is recognized by this last)\n",
        "    \"\"\"\n",
        "    raise NotImplementedError(\"Method parse not implemented\")\n",
        "\n",
        "\n",
        "class FSM(Machine):\n",
        "  \"\"\"\n",
        "  A class representing a non-deterministic Finite State Machine (FSM).\n",
        "  Each instance of this class in itself represent a single FSM state,\n",
        "  however multiple instance linked via transitions form non-deterministic FSMs\n",
        "  of arbitrary complexity.\n",
        "  The FSM instance of the first state can be used as an interface to the\n",
        "  whole FSM.\n",
        "  \"\"\"\n",
        "  def __init__(self, transitions : Optional[Dict[str, List[\"FSM\"]]] = None, is_terminal: bool = False):\n",
        "    \"\"\"\n",
        "    Create a new state instance.\n",
        "\n",
        "    transitions: optional dictionnary of transitions\n",
        "    is_terminal: indicates whether this state must be considered terminal (note\n",
        "      that a state with no transitions if already considered terminal by default)\n",
        "    \"\"\"\n",
        "    if transitions is None:\n",
        "      transitions = {}\n",
        "    self.transitions = transitions;\n",
        "    self._is_terminal_overwrite = is_terminal\n",
        "\n",
        "  def set_terminal(self, is_terminal: bool = True) -> None:\n",
        "    \"\"\"\n",
        "    If true forces this state to be considered as terminal\n",
        "    \"\"\"\n",
        "    self._is_terminal_overwrite = is_terminal;\n",
        "\n",
        "  def is_terminal(self) -> bool:\n",
        "    \"\"\"\n",
        "    Returns whether this state is terminal or not\n",
        "    \"\"\"\n",
        "    return self._is_terminal_overwrite or len(self.transitions) == 0;\n",
        "\n",
        "  def add_transition(self, input_symbol: int, states: List[\"FSM\"]) -> None:\n",
        "    \"\"\"\n",
        "    Add one or more new transitions for a given input symbol.\n",
        "    If there already exist transitions for that symbol, old transitions are kept.\n",
        "    Note that transitions can be duplicated.\n",
        "\n",
        "    input_symbol: an integer representing the input consumed by the FSM\n",
        "    states: list of states which we can transition to given input symbol\n",
        "    \"\"\"\n",
        "    self.transitions[input_symbol] = self.transitions.get(input_symbol, []) + states\n",
        "\n",
        "  def traverse(self, input_symbol: int) -> List[\"FSM\"]:\n",
        "    \"\"\"\n",
        "    Returns the list of states we can transition to given an input symbol\n",
        "    \"\"\"\n",
        "    return self.transitions.get(input_symbol, [])\n",
        "\n",
        "  def rand_traverse(self) -> Tuple[int, \"FSM\"]:\n",
        "    \"\"\"\n",
        "    Randomly pick a valid input symbol and a state to transition to\n",
        "    \"\"\"\n",
        "    input_symbol = random.choice(list(self.transitions.keys()))\n",
        "    next_state = random.choice(self.transitions[input_symbol])\n",
        "    return (input_symbol, next_state)\n",
        "\n",
        "  def rand_walk(self) -> Generator[int, None, None]:\n",
        "    \"\"\"\n",
        "    Randomly generate a sequence of input symbols accepted by the FSM\n",
        "    \"\"\"\n",
        "    state = self\n",
        "    while True:\n",
        "      if (not state.transitions) or (state.is_terminal() and random.randint(0,2) % 2 == 0):\n",
        "        return\n",
        "\n",
        "      input_symbol, state = state.rand_traverse()\n",
        "      yield input_symbol\n",
        "\n",
        "  def parse(self, expr: List[int], stop_symbol: Optional[int] = None) -> bool:\n",
        "    \"\"\"\n",
        "    Parse an expression and return True if it is well formed (i.e. it belongs to\n",
        "    the language encoded by the FSM and it is recognized by this last)\n",
        "    \"\"\"\n",
        "    if len(expr) == 0 or (stop_symbol is not None and expr[0] == stop_symbol):\n",
        "      return self.is_terminal()\n",
        "\n",
        "    states = self.traverse(expr[0])\n",
        "    for state in states:\n",
        "      if state.parse(expr[1:], stop_symbol=stop_symbol):\n",
        "        return True\n",
        "    return False\n",
        "\n",
        "  def bfs(self, max_samples: Optional[int] = None, max_depth: Optional[int] = None) -> Generator[List[int], None, None]:\n",
        "    \"\"\"\n",
        "    Generates up to max_samples well formed expressions beloging to the underlying\n",
        "    language (see gen).\n",
        "    \"\"\"\n",
        "    # Queue of tuples (expr, state)\n",
        "    queue = deque([([], self)])\n",
        "\n",
        "    while queue:\n",
        "      if max_samples is not None and max_samples <= 0:\n",
        "        return\n",
        "\n",
        "      expr, state = queue.popleft()\n",
        "\n",
        "      if state.is_terminal():\n",
        "        yield expr\n",
        "        if max_samples is not None:\n",
        "          max_samples -= 1\n",
        "\n",
        "      if max_depth is not None and len(expr) >= max_depth:\n",
        "        continue\n",
        "\n",
        "      for input_, states in state.transitions.items():\n",
        "        new_expr = expr + [input_]\n",
        "        for new_state in states:\n",
        "          queue.append((new_expr, new_state))\n",
        "\n",
        "  def gen(self, max_samples: Optional[int] = None, max_length: Optional[int] = None) -> Generator[List[int], None, None]:\n",
        "    return self.bfs(max_samples=max_samples, max_depth=max_length)\n",
        "\n",
        "\n",
        "# Some testing, just to make sure everything works correctly\n",
        "def test_fsm():\n",
        "  s = [FSM() for _ in range(3)]\n",
        "  s[0].add_transition(input_symbol=1, states=[s[0],s[1]])\n",
        "  s[1].add_transition(input_symbol=2, states=[s[0],s[2]])\n",
        "  s[2].add_transition(input_symbol=3, states=[s[2]])\n",
        "  s[2].set_terminal(True)\n",
        "\n",
        "  fsm = s[0]\n",
        "\n",
        "  # Test rand traverse\n",
        "  for _ in range(30):\n",
        "    expr = list(fsm.rand_walk())\n",
        "    if not fsm.parse(expr):\n",
        "      raise Exception(\"rand_traverse failed: \"+str(expr))\n",
        "\n",
        "  # Test bfs\n",
        "  for expr in fsm.gen(30):\n",
        "    if not fsm.parse(expr):\n",
        "      raise Exception(\"bfs failed: \"+str(expr))\n",
        "\n",
        "test_fsm()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z1t3JhEVQcBa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "461ed37b-6ee9-4b4d-81c5-91d63e8f04f1"
      },
      "source": [
        "\n",
        "class Production(object):\n",
        "  def __init__(self, *terms):\n",
        "    self.terms = terms\n",
        "  def __len__(self):\n",
        "    return len(self.terms)\n",
        "  def __getitem__(self, index):\n",
        "    return self.terms[index]\n",
        "  def __iter__(self):\n",
        "    return iter(self.terms)\n",
        "  def __repr__(self):\n",
        "    return \" \".join(str(t) for t in self.terms)\n",
        "  def __eq__(self, other):\n",
        "    if not isinstance(other, Production):\n",
        "      return False\n",
        "    return self.terms == other.terms\n",
        "  def __ne__(self, other):\n",
        "    return not (self == other)\n",
        "  def __hash__(self):\n",
        "    return hash(self.terms)\n",
        "\n",
        "class Rule(object):\n",
        "  def __init__(self, name, *productions):\n",
        "    self.name = name\n",
        "    self.productions = list(productions)\n",
        "  def __str__(self):\n",
        "    return self.name\n",
        "  def __repr__(self):\n",
        "    return \"%s -> %s\" % (self.name, \" | \".join(repr(p) for p in self.productions))\n",
        "  def add(self, *productions):\n",
        "    self.productions.extend(productions)\n",
        "\n",
        "class State(object):\n",
        "  def __init__(self, name, production, dot_index, start_column):\n",
        "    self.name = name\n",
        "    self.production = production\n",
        "    self.start_column = start_column\n",
        "    self.end_column = None\n",
        "    self.dot_index = dot_index\n",
        "    self.rules = [t for t in production if isinstance(t, Rule)]\n",
        "  def __repr__(self):\n",
        "    terms = [str(p) for p in self.production]\n",
        "    terms.insert(self.dot_index, u\"$\")\n",
        "    return \"%-5s -> %-16s [%s-%s]\" % (self.name, \" \".join(terms), self.start_column, self.end_column)\n",
        "  def __eq__(self, other):\n",
        "    return (self.name, self.production, self.dot_index, self.start_column) == \\\n",
        "      (other.name, other.production, other.dot_index, other.start_column)\n",
        "  def __ne__(self, other):\n",
        "    return not (self == other)\n",
        "  def __hash__(self):\n",
        "    return hash((self.name, self.production))\n",
        "  def completed(self):\n",
        "    return self.dot_index >= len(self.production)\n",
        "  def next_term(self):\n",
        "    if self.completed():\n",
        "      return None\n",
        "    return self.production[self.dot_index]\n",
        "\n",
        "class Column(object):\n",
        "  def __init__(self, index, token):\n",
        "    self.index = index\n",
        "    self.token = token\n",
        "    self.states = []\n",
        "    self._unique = set()\n",
        "  def __str__(self):\n",
        "    return str(self.index)\n",
        "  def __len__(self):\n",
        "    return len(self.states)\n",
        "  def __iter__(self):\n",
        "    return iter(self.states)\n",
        "  def __getitem__(self, index):\n",
        "    return self.states[index]\n",
        "  def enumfrom(self, index):\n",
        "    for i in range(index, len(self.states)):\n",
        "      yield i, self.states[i]\n",
        "  def add(self, state):\n",
        "    if state not in self._unique:\n",
        "      self._unique.add(state)\n",
        "      state.end_column = self\n",
        "      self.states.append(state)\n",
        "      return True\n",
        "    return False\n",
        "  def print_(self, completedOnly = False):\n",
        "    print(\"[%s] %r\" % (self.index, self.token))\n",
        "    print(\"=\" * 35)\n",
        "    for s in self.states:\n",
        "      if completedOnly and not s.completed():\n",
        "        continue\n",
        "      print(repr(s))\n",
        "    print(\"\")\n",
        "\n",
        "class Node(object):\n",
        "  def __init__(self, value, children):\n",
        "    self.value = value\n",
        "    self.children = children\n",
        "  def print_(self, level = 0):\n",
        "    print(\" \" * level + str(self.value))\n",
        "    for child in self.children:\n",
        "      child.print_(level + 1)\n",
        "\n",
        "def predict(col, rule):\n",
        "  for prod in rule.productions:\n",
        "    col.add(State(rule.name, prod, 0, col))\n",
        "\n",
        "def scan(col, state, token):\n",
        "  if token != col.token:\n",
        "    return\n",
        "  col.add(State(state.name, state.production, state.dot_index + 1, state.start_column))\n",
        "\n",
        "def complete(col, state):\n",
        "  if not state.completed():\n",
        "    return\n",
        "  for st in state.start_column:\n",
        "    term = st.next_term()\n",
        "    if not isinstance(term, Rule):\n",
        "      continue\n",
        "    if term.name == state.name:\n",
        "      col.add(State(st.name, st.production, st.dot_index + 1, st.start_column))\n",
        "\n",
        "GAMMA_RULE = u\"GAMMA\"\n",
        "\n",
        "def parse(rule, text):\n",
        "  table = [Column(i, tok) for i, tok in enumerate([None] + text.lower().split())]\n",
        "  table[0].add(State(GAMMA_RULE, Production(rule), 0, table[0]))\n",
        "\n",
        "  for i, col in enumerate(table):\n",
        "    for state in col:\n",
        "      if state.completed():\n",
        "        complete(col, state)\n",
        "      else:\n",
        "        term = state.next_term()\n",
        "        if isinstance(term, Rule):\n",
        "          predict(col, term)\n",
        "        elif i + 1 < len(table):\n",
        "          scan(table[i+1], state, term)\n",
        "\n",
        "\n",
        "  # find gamma rule in last table column (otherwise fail)\n",
        "  for st in table[-1]:\n",
        "    if st.name == GAMMA_RULE and st.completed():\n",
        "      return st\n",
        "  else:\n",
        "    raise ValueError(\"parsing failed\")\n",
        "\n",
        "def build_trees(state):\n",
        "  return build_trees_helper([], state, len(state.rules) - 1, state.end_column)\n",
        "\n",
        "def build_trees_helper(children, state, rule_index, end_column):\n",
        "  if rule_index < 0:\n",
        "    return [Node(state, children)]\n",
        "  elif rule_index == 0:\n",
        "    start_column = state.start_column\n",
        "  else:\n",
        "    start_column = None\n",
        "\n",
        "  rule = state.rules[rule_index]\n",
        "  outputs = []\n",
        "  for st in end_column:\n",
        "    if st is state:\n",
        "      break\n",
        "    if st is state or not st.completed() or st.name != rule.name:\n",
        "      continue\n",
        "    if start_column is not None and st.start_column != start_column:\n",
        "      continue\n",
        "    for sub_tree in build_trees(st):\n",
        "      for node in build_trees_helper([sub_tree] + children, state, rule_index - 1, st.start_column):\n",
        "        outputs.append(node)\n",
        "  return outputs\n",
        "\n",
        "\n",
        "SYM = Rule(\"SYM\", Production(\"a\"))\n",
        "OP = Rule(\"OP\", Production(\"+\"))\n",
        "EXPR = Rule(\"EXPR\", Production(SYM))\n",
        "EXPR.add(Production(EXPR, OP, EXPR))\n",
        "\n",
        "for i in range(1,9):\n",
        "  text = \" + \".join([\"a\"] * i)\n",
        "  q0 = parse(EXPR, text)\n",
        "  forest = build_trees(q0)\n",
        "  print(len(forest), text)\n",
        "\n",
        "\n",
        "N = Rule(\"N\", Production(\"time\"), Production(\"flight\"), Production(\"banana\"),\n",
        "  Production(\"flies\"), Production(\"boy\"), Production(\"telescope\"))\n",
        "D = Rule(\"D\", Production(\"the\"), Production(\"a\"), Production(\"an\"))\n",
        "V = Rule(\"V\", Production(\"book\"), Production(\"eat\"), Production(\"sleep\"), Production(\"saw\"))\n",
        "P = Rule(\"P\", Production(\"with\"), Production(\"in\"), Production(\"on\"), Production(\"at\"),\n",
        "  Production(\"through\"))\n",
        "\n",
        "PP = Rule(\"PP\")\n",
        "NP = Rule(\"NP\", Production(D, N), Production(\"john\"), Production(\"houston\"))\n",
        "NP.add(Production(NP, PP))\n",
        "PP.add(Production(P, NP))\n",
        "\n",
        "VP = Rule(\"VP\", Production(V, NP))\n",
        "VP.add(Production(VP, PP))\n",
        "S = Rule(\"S\", Production(NP, VP), Production(VP))\n",
        "\n",
        "for tree in build_trees(parse(S, \"book the flight through houston\")):\n",
        "  print(\"--------------------------\")\n",
        "  tree.print_()\n",
        "\n",
        "for tree in build_trees(parse(S, \"john saw the boy with the telescope\")):\n",
        "  print(\"--------------------------\")\n",
        "  tree.print_()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1 a\n",
            "1 a + a\n",
            "2 a + a + a\n",
            "5 a + a + a + a\n",
            "14 a + a + a + a + a\n",
            "42 a + a + a + a + a + a\n",
            "132 a + a + a + a + a + a + a\n",
            "429 a + a + a + a + a + a + a + a\n",
            "--------------------------\n",
            "GAMMA -> S $              [0-5]\n",
            "  S     -> VP $             [0-5]\n",
            "    VP    -> VP PP $          [0-5]\n",
            "      VP    -> V NP $           [0-3]\n",
            "        V     -> book $           [0-1]\n",
            "        NP    -> D N $            [1-3]\n",
            "          D     -> the $            [1-2]\n",
            "          N     -> flight $         [2-3]\n",
            "      PP    -> P NP $           [3-5]\n",
            "        P     -> through $        [3-4]\n",
            "        NP    -> houston $        [4-5]\n",
            "--------------------------\n",
            "GAMMA -> S $              [0-5]\n",
            "  S     -> VP $             [0-5]\n",
            "    VP    -> V NP $           [0-5]\n",
            "      V     -> book $           [0-1]\n",
            "      NP    -> NP PP $          [1-5]\n",
            "        NP    -> D N $            [1-3]\n",
            "          D     -> the $            [1-2]\n",
            "          N     -> flight $         [2-3]\n",
            "        PP    -> P NP $           [3-5]\n",
            "          P     -> through $        [3-4]\n",
            "          NP    -> houston $        [4-5]\n",
            "--------------------------\n",
            "GAMMA -> S $              [0-7]\n",
            "  S     -> NP VP $          [0-7]\n",
            "    NP    -> john $           [0-1]\n",
            "    VP    -> VP PP $          [1-7]\n",
            "      VP    -> V NP $           [1-4]\n",
            "        V     -> saw $            [1-2]\n",
            "        NP    -> D N $            [2-4]\n",
            "          D     -> the $            [2-3]\n",
            "          N     -> boy $            [3-4]\n",
            "      PP    -> P NP $           [4-7]\n",
            "        P     -> with $           [4-5]\n",
            "        NP    -> D N $            [5-7]\n",
            "          D     -> the $            [5-6]\n",
            "          N     -> telescope $      [6-7]\n",
            "--------------------------\n",
            "GAMMA -> S $              [0-7]\n",
            "  S     -> NP VP $          [0-7]\n",
            "    NP    -> john $           [0-1]\n",
            "    VP    -> V NP $           [1-7]\n",
            "      V     -> saw $            [1-2]\n",
            "      NP    -> NP PP $          [2-7]\n",
            "        NP    -> D N $            [2-4]\n",
            "          D     -> the $            [2-3]\n",
            "          N     -> boy $            [3-4]\n",
            "        PP    -> P NP $           [4-7]\n",
            "          P     -> with $           [4-5]\n",
            "          NP    -> D N $            [5-7]\n",
            "            D     -> the $            [5-6]\n",
            "            N     -> telescope $      [6-7]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZVl9w36BEdQz"
      },
      "source": [
        "Let's have a class to generate all datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V_iuKHWWtIjL"
      },
      "source": [
        "class DatasetUtils:\n",
        "\n",
        "  @staticmethod\n",
        "  def fsm_test1():\n",
        "    fsm, alphabet_size = DatasetUtils.toy_fsm()\n",
        "    return DatasetUtils.make_dataset(\n",
        "        machine=fsm,\n",
        "        alphabet_size=alphabet_size,\n",
        "        max_length=10,\n",
        "        max_samples=300,\n",
        "    )\n",
        "\n",
        "  @staticmethod\n",
        "  def toy_fsm() -> Tuple[FSM, int]:\n",
        "    \"\"\" A very simple FSM \"\"\"\n",
        "    s = [FSM(is_terminal=True) for _ in range(3)]\n",
        "    s[0].add_transition(input_symbol=1, states=[s[0],s[1]])\n",
        "    s[1].add_transition(input_symbol=2, states=[s[1],s[2]])\n",
        "    s[2].add_transition(input_symbol=3, states=[s[2],s[0]])\n",
        "    return (s[0], 3)\n",
        "\n",
        "  @staticmethod\n",
        "  def make_dataset(machine: Machine, alphabet_size: int, max_length: int, max_samples: int) -> Dict:\n",
        "    \"\"\"\n",
        "    Create a dataset from a given machine\n",
        "\n",
        "    machine: instance of a machine\n",
        "    alphabet_size: size of the alphabet used by the machine\n",
        "    max_length: max size of the expressions\n",
        "    max_samples: max number of samples contained in the dataset\n",
        "    \"\"\"\n",
        "    # Generate samples using and add zero padding\n",
        "    # To increase variance we generate num_samples*2 samples and randomly\n",
        "    # pick num_samples\n",
        "    X = list(machine.gen(max_samples=max_samples*2, max_length=max_length))\n",
        "    X = random.sample(X, k=min(max_samples, len(X)))\n",
        "\n",
        "    # Zero-pad to max_length and stack samples together\n",
        "    for i in range(len(X)):\n",
        "      X[i].extend([0]*(max_length-len(X[i])))\n",
        "    X = tf.stack(X)\n",
        "\n",
        "    # Add a leading zero indicating the end of the sequence\n",
        "    X = tf.pad(X, tf.constant([[0,0],[1,0]]))\n",
        "\n",
        "    # Labels, like X but one step ahead and zero-padded\n",
        "    Y = tf.pad(X[:,1:], tf.constant([[0,0],[0,1]]))\n",
        "\n",
        "    # Training data\n",
        "    X = tf.one_hot(indices=X, depth=alphabet_size+1) # shape(M, Tx, alphabet_size+1)\n",
        "    Y = tf.one_hot(indices=Y, depth=alphabet_size+1)\n",
        "\n",
        "    return {\n",
        "      'M': X.shape[0],\n",
        "      'Tx': X.shape[1],\n",
        "      'X': X,\n",
        "      'Y': Y,\n",
        "      'machine': machine,\n",
        "      'alphabet_size': alphabet_size+1\n",
        "    }"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N_XvNei-3M2j"
      },
      "source": [
        "class TestUtils:\n",
        "\n",
        "  @staticmethod\n",
        "  def sample_categorical(preds, temperature: float = 1.0, num_samples: int = 1):\n",
        "    preds = tf.math.log(preds)/temperature\n",
        "    return tf.random.categorical(preds,num_samples=num_samples)\n",
        "\n",
        "  @staticmethod\n",
        "  def generate(\n",
        "      model: keras.Model,\n",
        "      alphabet_size: int,\n",
        "      num_samples: int = 1,\n",
        "      max_length: int = 20,\n",
        "      temperature: float = 1.0):\n",
        "    samples = tf.one_hot([[0,]]*num_samples, depth=alphabet_size)\n",
        "    for _ in range(max_length):\n",
        "      preds = model(samples)\n",
        "      preds = TestUtils.sample_categorical(preds[:,-1,:],temperature=temperature)\n",
        "      preds = tf.one_hot(preds, depth=alphabet_size)\n",
        "      samples = tf.concat([samples,preds],axis=1)\n",
        "    return tf.argmax(samples, axis=2)\n",
        "\n",
        "  @staticmethod\n",
        "  def clean_exprs(exprs: np.array):\n",
        "    exprs = exprs[:, 1:]\n",
        "    for e in exprs:\n",
        "      end_expr = np.argwhere(e == 0)[0][0]\n",
        "      e[end_expr:] = 0\n",
        "    return exprs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i_B77wnbBpdG"
      },
      "source": [
        "# Part 2 - Vanilla LSTM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ujo_1qIQEl1B"
      },
      "source": [
        "We will start by testing out the performance of a vanilla LSTM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gHB10oDQHnZa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "47639e6d-163c-4a81-8278-4038dbb646e3"
      },
      "source": [
        "def vanilla_lstm(Tx, alphabet_size):\n",
        "  X = keras.layers.Input(shape=(Tx, alphabet_size))\n",
        "  tmp = keras.layers.LSTM(30, return_sequences=True)(X)\n",
        "  tmp = keras.layers.Dense(alphabet_size, activation='softmax')(tmp)\n",
        "\n",
        "  model = keras.Model(inputs=[X],outputs=[tmp])\n",
        "  model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "  return model\n",
        "\n",
        "settings = DatasetUtils.fsm_test1()\n",
        "model = vanilla_lstm(settings['Tx'], settings['alphabet_size'])\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"functional_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         [(None, 11, 4)]           0         \n",
            "_________________________________________________________________\n",
            "lstm (LSTM)                  (None, 11, 30)            4200      \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 11, 4)             124       \n",
            "=================================================================\n",
            "Total params: 4,324\n",
            "Trainable params: 4,324\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_oZiDGUOmKZz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2da6d91c-8aae-470f-da34-5b8f43a05426"
      },
      "source": [
        "model.fit(x=settings['X'],y=settings['Y'], epochs=100, batch_size=20, verbose=1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 1.3569 - accuracy: 0.4042\n",
            "Epoch 2/100\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 1.2792 - accuracy: 0.4288\n",
            "Epoch 3/100\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 1.1879 - accuracy: 0.4291\n",
            "Epoch 4/100\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 1.1229 - accuracy: 0.4945\n",
            "Epoch 5/100\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 1.0579 - accuracy: 0.5658\n",
            "Epoch 6/100\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 1.0000 - accuracy: 0.6176\n",
            "Epoch 7/100\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.9521 - accuracy: 0.6470\n",
            "Epoch 8/100\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.9108 - accuracy: 0.6627\n",
            "Epoch 9/100\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.8788 - accuracy: 0.6621\n",
            "Epoch 10/100\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.8537 - accuracy: 0.6676\n",
            "Epoch 11/100\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.8334 - accuracy: 0.6718\n",
            "Epoch 12/100\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.8185 - accuracy: 0.6709\n",
            "Epoch 13/100\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.8008 - accuracy: 0.6730\n",
            "Epoch 14/100\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.7876 - accuracy: 0.6733\n",
            "Epoch 15/100\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.7786 - accuracy: 0.6736\n",
            "Epoch 16/100\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.7608 - accuracy: 0.6758\n",
            "Epoch 17/100\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.7533 - accuracy: 0.6794\n",
            "Epoch 18/100\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.7510 - accuracy: 0.6803\n",
            "Epoch 19/100\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.7362 - accuracy: 0.6779\n",
            "Epoch 20/100\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.7259 - accuracy: 0.6794\n",
            "Epoch 21/100\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.7191 - accuracy: 0.6776\n",
            "Epoch 22/100\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.7090 - accuracy: 0.6806\n",
            "Epoch 23/100\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.6966 - accuracy: 0.6776\n",
            "Epoch 24/100\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.6899 - accuracy: 0.6791\n",
            "Epoch 25/100\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.6829 - accuracy: 0.6773\n",
            "Epoch 26/100\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.6785 - accuracy: 0.6791\n",
            "Epoch 27/100\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.6706 - accuracy: 0.6791\n",
            "Epoch 28/100\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.6625 - accuracy: 0.6803\n",
            "Epoch 29/100\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.6593 - accuracy: 0.6785\n",
            "Epoch 30/100\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.6521 - accuracy: 0.6785\n",
            "Epoch 31/100\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.6464 - accuracy: 0.6788\n",
            "Epoch 32/100\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.6415 - accuracy: 0.6773\n",
            "Epoch 33/100\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.6437 - accuracy: 0.6782\n",
            "Epoch 34/100\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.6338 - accuracy: 0.6800\n",
            "Epoch 35/100\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.6296 - accuracy: 0.6773\n",
            "Epoch 36/100\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.6252 - accuracy: 0.6770\n",
            "Epoch 37/100\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.6197 - accuracy: 0.6782\n",
            "Epoch 38/100\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.6159 - accuracy: 0.6800\n",
            "Epoch 39/100\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.6123 - accuracy: 0.6758\n",
            "Epoch 40/100\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.6082 - accuracy: 0.6767\n",
            "Epoch 41/100\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.6069 - accuracy: 0.6803\n",
            "Epoch 42/100\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.6016 - accuracy: 0.6806\n",
            "Epoch 43/100\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.6002 - accuracy: 0.6791\n",
            "Epoch 44/100\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.5983 - accuracy: 0.6800\n",
            "Epoch 45/100\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.5949 - accuracy: 0.6818\n",
            "Epoch 46/100\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.5959 - accuracy: 0.6797\n",
            "Epoch 47/100\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.5899 - accuracy: 0.6788\n",
            "Epoch 48/100\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.5936 - accuracy: 0.6818\n",
            "Epoch 49/100\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.5907 - accuracy: 0.6806\n",
            "Epoch 50/100\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.5829 - accuracy: 0.6885\n",
            "Epoch 51/100\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.5798 - accuracy: 0.6833\n",
            "Epoch 52/100\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.5771 - accuracy: 0.6824\n",
            "Epoch 53/100\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.5762 - accuracy: 0.6858\n",
            "Epoch 54/100\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.5733 - accuracy: 0.6867\n",
            "Epoch 55/100\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.5713 - accuracy: 0.6885\n",
            "Epoch 56/100\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.5708 - accuracy: 0.6891\n",
            "Epoch 57/100\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.5696 - accuracy: 0.6852\n",
            "Epoch 58/100\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.5714 - accuracy: 0.6842\n",
            "Epoch 59/100\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.5641 - accuracy: 0.6912\n",
            "Epoch 60/100\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.5645 - accuracy: 0.6885\n",
            "Epoch 61/100\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.5626 - accuracy: 0.6891\n",
            "Epoch 62/100\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.5666 - accuracy: 0.6924\n",
            "Epoch 63/100\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.5629 - accuracy: 0.6930\n",
            "Epoch 64/100\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.5591 - accuracy: 0.6848\n",
            "Epoch 65/100\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.5584 - accuracy: 0.6897\n",
            "Epoch 66/100\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.5567 - accuracy: 0.6918\n",
            "Epoch 67/100\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.5561 - accuracy: 0.6964\n",
            "Epoch 68/100\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.5595 - accuracy: 0.6900\n",
            "Epoch 69/100\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.5552 - accuracy: 0.6903\n",
            "Epoch 70/100\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.5530 - accuracy: 0.6921\n",
            "Epoch 71/100\n",
            "15/15 [==============================] - 0s 6ms/step - loss: 0.5509 - accuracy: 0.6912\n",
            "Epoch 72/100\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.5535 - accuracy: 0.6964\n",
            "Epoch 73/100\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.5512 - accuracy: 0.6939\n",
            "Epoch 74/100\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.5497 - accuracy: 0.6942\n",
            "Epoch 75/100\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.5493 - accuracy: 0.6973\n",
            "Epoch 76/100\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.5529 - accuracy: 0.6945\n",
            "Epoch 77/100\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.5517 - accuracy: 0.6906\n",
            "Epoch 78/100\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.5464 - accuracy: 0.6958\n",
            "Epoch 79/100\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.5472 - accuracy: 0.6967\n",
            "Epoch 80/100\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.5468 - accuracy: 0.6930\n",
            "Epoch 81/100\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.5473 - accuracy: 0.6942\n",
            "Epoch 82/100\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.5458 - accuracy: 0.6903\n",
            "Epoch 83/100\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.5434 - accuracy: 0.6967\n",
            "Epoch 84/100\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.5431 - accuracy: 0.6952\n",
            "Epoch 85/100\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.5444 - accuracy: 0.6933\n",
            "Epoch 86/100\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.5438 - accuracy: 0.6982\n",
            "Epoch 87/100\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.5445 - accuracy: 0.6964\n",
            "Epoch 88/100\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.5414 - accuracy: 0.6948\n",
            "Epoch 89/100\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.5411 - accuracy: 0.6994\n",
            "Epoch 90/100\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.5418 - accuracy: 0.6976\n",
            "Epoch 91/100\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.5397 - accuracy: 0.6964\n",
            "Epoch 92/100\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.5396 - accuracy: 0.6955\n",
            "Epoch 93/100\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.5400 - accuracy: 0.6939\n",
            "Epoch 94/100\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.5396 - accuracy: 0.6979\n",
            "Epoch 95/100\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.5395 - accuracy: 0.7009\n",
            "Epoch 96/100\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.5387 - accuracy: 0.6967\n",
            "Epoch 97/100\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.5390 - accuracy: 0.6973\n",
            "Epoch 98/100\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.5380 - accuracy: 0.6991\n",
            "Epoch 99/100\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.5371 - accuracy: 0.6976\n",
            "Epoch 100/100\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.5384 - accuracy: 0.6988\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f23ca8ccd30>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2xLBfrEumYRt",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "outputId": "18cd9cb9-8bb1-46b1-b0e2-1b04dd72a8e5"
      },
      "source": [
        "plt.plot(model.history.history['loss'])\n",
        "plt.xlabel('No. epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NLPUhaeeTMYG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9b99ae3c-2731-4ed0-8cf2-0087ff9bda39"
      },
      "source": [
        "print(\"Example of generated sequences\")\n",
        "TestUtils.generate(model, settings['alphabet_size'], 10, 20)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Example of generated sequences\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(10, 21), dtype=int64, numpy=\n",
              "array([[0, 1, 1, 1, 1, 1, 2, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
              "       [0, 1, 1, 1, 2, 2, 2, 3, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
              "       [0, 1, 1, 1, 2, 2, 3, 1, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
              "       [0, 1, 2, 3, 1, 1, 1, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
              "       [0, 1, 1, 1, 2, 3, 3, 1, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
              "       [0, 1, 2, 2, 2, 2, 2, 2, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
              "       [0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
              "       [0, 1, 1, 1, 2, 2, 2, 2, 2, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
              "       [0, 1, 2, 2, 2, 2, 2, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
              "       [0, 1, 1, 2, 2, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PH2Tl7PwtuSR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fef92154-ab4c-4711-8618-cf4a4b34ccfa"
      },
      "source": [
        "exprs = TestUtils.generate(model, settings['alphabet_size'], num_samples=100, max_length=100, temperature=1.0)\n",
        "exprs = TestUtils.clean_exprs(exprs.numpy())\n",
        "\n",
        "ill_formed = []\n",
        "for expr in exprs:\n",
        "  if not settings['machine'].parse(expr, stop_symbol=0):\n",
        "    ill_formed.append(expr)\n",
        "print(f\"Well formed expressions: {len(exprs)-len(ill_formed)}/{len(exprs)}\")\n",
        "\n",
        "num_uniques = np.unique(exprs, axis=0).shape[0]\n",
        "print(f\"Unique expressions: {num_uniques}/{len(exprs)}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Well formed expressions: 93/100\n",
            "Unique expressions: 84/100\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BIEeDWBq1stK"
      },
      "source": [
        "# Random ideas\n",
        "\n",
        "- Try a net architecture involving a \"fixer\", for example:\n",
        " - LSTM A generates an expression\n",
        " - LSTM B reads the generated expression and then outputs a new \"fixed\" expression\n",
        "   based on the input.\n",
        " - The idea here is that B has a global view of the expression read and needs only focus on localized actions. So in theory it would be a much easier job.\n",
        " - There can be multiple iterations of B\n",
        "- Try reinforcement learning\n",
        "- GAN\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oqoH3Or6q7Ih"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    }
  ]
}